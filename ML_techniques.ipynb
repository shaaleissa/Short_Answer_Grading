{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-Proccesing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing the pandas library for data manipulation\n",
    "import re  # Importing the re module for regular expression operations\n",
    "\n",
    "def read_and_split_file(file_path):\n",
    "    # List of common file encodings to try reading the file\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'latin1', 'cp1252']\n",
    "    \n",
    "    # Attempt to read the file with each encoding\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Open the file with the current encoding\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.readlines()  # Read all lines of the file\n",
    "\n",
    "            data = []  # Initialize a list to store processed data\n",
    "            # Process each line in the file\n",
    "            for line in content:\n",
    "                # Use regular expression to split the line at the first occurrence of a number pattern\n",
    "                split_line = re.split(r'(\\d+\\.\\d+)\\s+', line, maxsplit=1)\n",
    "                # If the line is successfully split into three parts (part before number, number, part after number)\n",
    "                if len(split_line) == 3:\n",
    "                    number, answer = split_line[1], split_line[2]  # Extract the number and the answer\n",
    "                    data.append([number, answer.strip()])  # Add to the data list\n",
    "            return data  # Return the processed data\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            # If the current encoding fails, continue to try with the next encoding\n",
    "            continue  \n",
    "\n",
    "    # If all encodings fail, raise an error indicating the file could not be decoded\n",
    "    raise ValueError(f\"Unable to decode file {file_path} with given encodings.\")\n",
    "\n",
    "# File path for the data file\n",
    "student_answers_file = 'ShortAnswerGrading_v2/data/raw/all'\n",
    "\n",
    "# Read and split the file to get the student data\n",
    "student_data = read_and_split_file(student_answers_file)\n",
    "\n",
    "# Create a DataFrame from the student data with specified column names\n",
    "df_student = pd.DataFrame(student_data, columns=['Number', 'Student Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_answers_file = 'ShortAnswerGrading_v2/data/raw/answers'\n",
    "questions_file = 'ShortAnswerGrading_v2/data/raw/questions'\n",
    "\n",
    "# Read and split file\n",
    "instructor_data = read_and_split_file(instructor_answers_file)\n",
    "questions_data = read_and_split_file(questions_file)\n",
    "\n",
    "# Create DataFrames\n",
    "df_instructor_answers = pd.DataFrame(instructor_data, columns=['Number', 'Instructor Answer'])\n",
    "df_questions = pd.DataFrame(questions_data, columns=['Number', 'Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(df_questions, df_instructor_answers, on='Number')\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(combined_df, df_student, on='Number')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import os  # Importing os module for operating system dependent functionality\n",
    "\n",
    "def add_grades_to_df(df, base_dir):\n",
    "    \"\"\"\n",
    "    This function adds grades to a DataFrame from text files located in numbered directories.\n",
    "    It expects a DataFrame with a 'Number' column indicating directory names, and a base directory \n",
    "    path where these numbered directories are located.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterating through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Constructing the directory name from the 'Number' column in the DataFrame\n",
    "        directory_name = str(row['Number'])\n",
    "        # Creating the file path by joining the base directory, directory name, and a file named 'ave'\n",
    "        file_path = os.path.join(base_dir, directory_name, 'ave')\n",
    "        \n",
    "        # Checking if the constructed file path exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Opening and reading the contents of the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_contents = file.read().strip()  # Removing any leading/trailing whitespace\n",
    "\n",
    "                # Splitting the contents by newline and filtering out empty strings\n",
    "                grades = file_contents.split('\\n')\n",
    "                grades = [grade for grade in grades if grade]  \n",
    "\n",
    "                # Check if there are any grades in the list\n",
    "                if grades:\n",
    "                    # Selecting the first grade as the required grade\n",
    "                    selected_grade = grades[0]\n",
    "                    # Updating the DataFrame with the selected grade at the corresponding index\n",
    "                    df.at[index, 'grade'] = selected_grade\n",
    "                else:\n",
    "                    # Print a message if no valid grades are found in the file\n",
    "                    print(f\"No valid grades found in {file_path}\")\n",
    "    # Returning the modified DataFrame with grades added\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset_updated = add_grades_to_df(dataset, 'ShortAnswerGrading_v2/data/scores')\n",
    "# 'dataset_updated' now contains the original data along with added grades\n",
    "dataset_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_updated.to_csv('Dataset/dataset_with_grades.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "df=pd.read_csv('Dataset/dataset_with_grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing and removing non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "# Tokenizing\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Removing stopwords and lemmatizing\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "# Applying preprocessing to each text column\n",
    "df['Processed Question'] = df['Question'].apply(preprocess_text)\n",
    "df['Processed Instructor Answer'] = df['Instructor Answer'].apply(preprocess_text)\n",
    "df['Processed Student Answer'] = df['Student Answer'].apply(preprocess_text)\n",
    "\n",
    "# Combine all processed text into a single series for training the model\n",
    "all_text = pd.concat([df['Processed Question'], df['Processed Instructor Answer'], df['Processed Student Answer']])\n",
    "\n",
    "# Split the dataset (80% train, 20% test)\n",
    "train_text, test_text = train_test_split(all_text, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=all_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Models/word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def cosine_similarity_between_vectors(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "# Calculate average vectors for each answer\n",
    "df['inst_vector'] = df['Instructor Answer'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "df['student_vector'] = df['Student Answer'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "df['cosine_similarity'] = df.apply(lambda row: cosine_similarity_between_vectors(row['inst_vector'], row['student_vector']), axis=1)\n",
    "\n",
    "# Resulting DataFrame\n",
    "cosine_similarity_df = df[['Instructor Answer', 'Student Answer', 'cosine_similarity']]\n",
    "\n",
    "#calculate average cosine similarity\n",
    "print(\"Average cosine similarity between instructor and student answers: \", cosine_similarity_df['cosine_similarity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buliding a regressor with RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained word2vec model\n",
    "model = Word2Vec.load(\"Models/word2vec_model.model\")\n",
    "\n",
    "\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Feature extraction\n",
    "df['inst_vector'] = df['Processed Instructor Answer'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "df['student_vector'] = df['Processed Student Answer'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "\n",
    "# Combine vectors\n",
    "df['combined_vector'] = df.apply(lambda row: np.concatenate([row['inst_vector'], row['student_vector']]), axis=1)\n",
    "\n",
    "# Prepare dataset\n",
    "X = np.stack(df['combined_vector'])\n",
    "y = df['grade']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train model\n",
    "regressor = RandomForestRegressor() \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained word2vec and regressor models\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"Models/word2vec_model.model\")\n",
    "regressor_model = joblib.load(\"Models/regressor_for_shahad.joblib\")\n",
    "\n",
    "def predict_grade(ref_answer, student_answer, word2vec_model, regressor_model):\n",
    "    ref_vector = sentence_to_avg_vector(ref_answer, word2vec_model)\n",
    "    student_vector = sentence_to_avg_vector(student_answer, word2vec_model)\n",
    "    combined_vector = np.concatenate([ref_vector, student_vector])\n",
    "    predicted_grade = regressor_model.predict([combined_vector])[0]\n",
    "    return predicted_grade\n",
    "\n",
    "# Sample reference answer and student answer for testing\n",
    "sample_ref_answer = \"log n is the height of the tree.\"\n",
    "sample_student_answer = \"log n\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_grade = predict_grade(sample_ref_answer, sample_student_answer, word2vec_model, regressor_model)\n",
    "print(f\"Predicted Grade: {predicted_grade}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the random forest\n",
    "    'max_depth': [None, 10, 20, 30],        # Maximum number of levels in each tree\n",
    "    'min_samples_split': [2, 5, 10]   # Minimum number of samples required to split a node\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "predicted_grade = predict_grade(sample_ref_answer, sample_student_answer, word2vec_model, best_model)\n",
    "print(f\"Predicted Grade: {predicted_grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(regressor, 'regressor_for_shahad.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
